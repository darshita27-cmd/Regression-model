{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Supervised Learning Models:Regression Models"
      ],
      "metadata": {
        "id": "mpoBZGCSgpfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------step 1: to import libraries-------------------------\n",
        "import pandas as pd  # panda is used for data manipulation while working with datasets\n",
        "import numpy as np   # numpy is used for numerical operations like in array and matrixes\n",
        "import matplotlib.pyplot as plt   # matplotlib.pyplot is used for visualisation\n",
        "import seaborn as sns    # seaborn is used with matplotlib.pyplot. it gives a better interface for drawing, graphics\n",
        "from sklearn.model_selection import train_test_split   # used to slpit dataset into trai and test\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV    #  to create the linear regression model. relationship between dependent and independent variable\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error # this is used to evaluate the performance the linear regression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
      ],
      "metadata": {
        "id": "Rj_3_N0Ik2gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------step 2: to read csv --------------------------------\n",
        "df= pd.read_csv(\"/content/Housing.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "g7T_Rr_PnqDV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------step 3: data exploration---------------------------------\n",
        "print(df.info())    # this is used to get the insights of the data. it gives the number of rows, number of columns, data-tuype of the columns, help to find if there are missing values in the dataset\n",
        "print(df.describe())   # makes the descrptive statical analisis of data. it gives the count of non-null values, mean of each col, standard deviasion of each col, min value of each col, 25% dataset falls (first quaritile), 50% median (second quaritile), the values below which 75% of data falls (third quaritile), max value of each col"
      ],
      "metadata": {
        "id": "rYdh2ScloTjk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=[ 'area', 'bedrooms',\t'bathrooms',\t'stories']  # independent variables\n",
        "target=['price']   # dependent variable\n",
        "x=df[features]\n",
        "y=df[target]"
      ],
      "metadata": {
        "id": "0o3GdoAk9JtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------step 4: data cleaning and preprocessing -----------------------\n",
        "df= df.dropna(subset=['price']) # dropna removes the row that contains atleast one missing. subset specify if 'price' is NaN(not a number) then the row will be deleted.\n",
        "numeric_cols=df.select_dtypes(include=np.number).columns\n",
        "df[numeric_cols]=df[numeric_cols].fillna(df[numeric_cols].mean()) # this will fill the mean value in place of missing values. we are selecting numeric_cols to fill the mean values.fillna is used to fill the missing values.df.mean will calculate the mean of each col and when used with fillna it will fill the mean value in the missing places, inplace='True' is used to modify the dataframe directly rather than returning a new dataframe"
      ],
      "metadata": {
        "id": "4bASZI4xr83h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------step: 5: data visualization---------------------------------\n",
        "sns.pairplot(df[features+target])   # creates the a grid of scatter plots\n",
        "plt.show()   # display the plots"
      ],
      "metadata": {
        "id": "ABbkbVO_8WYe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------step 6: to split the data into test andt train-------------------------\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7kQlHbrI_OM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------step 7: build the regression model----------------------\n",
        "poly = PolynomialFeatures(degree=5)  # helps the model to learn quadraticaly\n",
        "x_train_poly=poly.fit_transform(x_train)  # fit calculates necessay parameters like number of features.  transform generates the polynomial new features example: kf there are features like A and B than it will create a new features as A*B\n",
        "x_test_poly=poly.transform(x_test)\n",
        "linear_regression_model= LinearRegression().fit(x_train,y_train)\n",
        "polynomial_regression_model=LinearRegression().fit(x_train_poly, y_train)\n",
        "\n",
        "##################### cheking the best alpha value##################\n",
        "ridge_cv=RidgeCV(alphas=[0.01,0.1,1,10,100], store_cv_values=True)\n",
        "ridge_cv.fit(x_train, y_train)\n",
        "print(\"best alpha value for ridge = \", ridge_cv.alpha_)\n",
        "\n",
        "lasso_cv=LassoCV(alphas=[0.01,0.1,1.0,10,100], cv=5)\n",
        "lasso_cv.fit(x_train, y_train)\n",
        "print(\"best apha value for lasso regression = \", lasso_cv.alpha_)\n",
        "###########################################################\n",
        "\n",
        "ridge_regression_model=Ridge(alpha=1.0).fit(x_train_poly, y_train)\n",
        "lasso_regression_model=Lasso(alpha=100).fit(x_train_poly, y_train)"
      ],
      "metadata": {
        "id": "N3aw8V4ABYqk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------step 8:make predections --------------------------------\n",
        "y_predict_linear_regression=linear_regression_model.predict(x_test)\n",
        "y_predict_polynomial_regression=polynomial_regression_model.predict(x_test_poly)\n",
        "y_predict_ridge_regression=ridge_regression_model.predict(x_test_poly)\n",
        "y_predict_lasso_regresson=lasso_regression_model.predict(x_test_poly)"
      ],
      "metadata": {
        "id": "K-atYzySBpsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------step 9: evaluate the model's performance --------------------------------\n",
        "mae_linear_regression=mean_absolute_error(y_test, y_predict_linear_regression)  # mean_absolute_error, mean_squared_error, r2_score is used to quntify the accuracy of regression model by measuring the average magnitute errors in predictions\n",
        "mse_linear_regression=mean_squared_error(y_test, y_predict_linear_regression)\n",
        "r2_linear_regression=r2_score(y_test, y_predict_linear_regression)\n",
        "print(f'mean_absolute_error_linear_regression ={mae_linear_regression}')\n",
        "print(f'mean_squared_error_linear_regression={mse_linear_regression}')\n",
        "print(f'r2_score_linear_regression={r2_linear_regression}')\n",
        "\n",
        "mae_polynomial_regression=mean_absolute_error(y_test, y_predict_polynomial_regression)\n",
        "mse_polynomial_regression=mean_squared_error(y_test, y_predict_polynomial_regression)\n",
        "r2_polynomial_regression=r2_score(y_test, y_predict_polynomial_regression)\n",
        "print(f'mean_absolute_error_polynomial_regression ={mae_polynomial_regression}')\n",
        "print(f'mean_squared_error_polynomial_regression={mse_polynomial_regression}')\n",
        "print(f'r2_score_polynomial_regression={r2_polynomial_regression}')\n",
        "\n",
        "mae_ridge_regression=mean_absolute_error(y_test, y_predict_ridge_regression)\n",
        "mse_ridge_regression=mean_squared_error(y_test, y_predict_ridge_regression)\n",
        "r2_ridge_regression=r2_score(y_test, y_predict_ridge_regression)\n",
        "print(f\"mean_absolute_error_ridge_regression={mae_ridge_regression}\")\n",
        "print(f\"mean_squared_error_ridge_regression={mse_ridge_regression}\")\n",
        "print(f\"r2_score_ridge_regression={r2_ridge_regression}\")\n",
        "\n",
        "mae_lasso_regression=mean_absolute_error(y_test, y_predict_lasso_regresson)\n",
        "mse_lasso_regression=mean_squared_error(y_test, y_predict_lasso_regresson)\n",
        "r2_lasso_regression=r2_score(y_test, y_predict_lasso_regresson)\n",
        "print(f\"mean_absolute_error_lasso_regression={mae_lasso_regression}\")\n",
        "print(f\"mean_squared_error_lasso_regression={mse_lasso_regression}\")\n",
        "print(f\"r2_score_lasso_regression={r2_lasso_regression}\")"
      ],
      "metadata": {
        "id": "4Bpnn0AFB-dz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------step 10: visualize the predictions -----------------\n",
        "\n",
        "plt.subplot(1,4,1)\n",
        "plt.scatter(y_test, y_predict_linear_regression)    # making a scatter plot\n",
        "plt.xlabel('actual price')\n",
        "plt.ylabel('predicted price')\n",
        "plt.title('actual vs predicted price')\n",
        "\n",
        "\n",
        "print(\"polynomial regression\")\n",
        "plt.subplot(1,4,2)   #   syntax (number of rows in subplot, number of columns of subplot, activate the subplot of the grid)\n",
        "plt.scatter(y_test, y_predict_polynomial_regression)\n",
        "plt.xlabel('actual price')\n",
        "plt.ylabel('predicted price')\n",
        "plt.title('actual vs predicted price')\n",
        "\n",
        "print(\"for ridge regression\")\n",
        "plt.subplot(1,4,3)\n",
        "plt.scatter(y_test, y_predict_ridge_regression)\n",
        "plt.xlabel('actual price')\n",
        "plt.ylabel('predicted price')\n",
        "plt.title('actual vs predicted price')\n",
        "\n",
        "\n",
        "print(\"for lasso regression\")\n",
        "plt.subplot(1,4,4)\n",
        "plt.scatter(y_test, y_predict_lasso_regresson)\n",
        "plt.xlabel('actual price')\n",
        "plt.ylabel('predicted price')\n",
        "plt.title('actual vs predicted price')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8BlMB9dTEnSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"linear regression=\",linear_regression_model.score(x_test, y_test))  # for linear regression\n",
        "print(\"polynomial regression=\",polynomial_regression_model.score(x_test_poly, y_test))  # for polynomial regression\n",
        "print(\"ridge regression=\",ridge_regression_model.score(x_test_poly, y_test))  # for ridge regression\n",
        "print(\"lasso regression=\",lasso_regression_model.score(x_test_poly, y_test))  # for lasso regression"
      ],
      "metadata": {
        "id": "DYcMw0HaF7es"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}